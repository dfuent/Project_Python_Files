{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3: Data Cleansing and Creation of Final Data File\n",
    "\n",
    "## Polisticians Semester Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need to create a function to be used on the DF to further clean up our speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speaker_map(row): # function to find if a transcript row contains a speaker\n",
    "\n",
    "    if type(row['Transcript']) is str: # if a transcript line is a string\n",
    "        \n",
    "        # there are two main instances of whether a transcript line contains\n",
    "        # a speaker: if there's a colon in the line, or if there's a semi-colon\n",
    "        # There are some other instances, but we clean those up later via \n",
    "        # a mapping file \n",
    "        \n",
    "        if row['Transcript'].find(':') > 0: # find returns -1 if ':' isn't found\n",
    "            f = row['Transcript'].find(':') # return the index of the colon\n",
    "        else: \n",
    "            f = row['Transcript'].find(';') # same as above except w/ semi-colon\n",
    "        if f > 0:            \n",
    "            return row['Transcript'][0:f] # if either ';' or ':' were found, return the name\n",
    "        else:\n",
    "            return np.nan # if neither found, return nan\n",
    "\n",
    "# there are some lines in the transcripts that have colons when a speaker is making a point (i.e. not a name). We want to \n",
    "# flag where the speaker_map returns a name, not one of these instances. We use this later to pull out the actual \n",
    "# transcript words and not just the name\n",
    "\n",
    "def find_colon(row): \n",
    "    i = 0\n",
    "    try:\n",
    "        for r in row['Transcript'].split(' '): \n",
    "            i += 1\n",
    "            if r.find(':') > 0:\n",
    "                \n",
    "                # return the position of the colon if it's found in one of the\n",
    "                # first 4 words\n",
    "                if i < 4:\n",
    "                    return i \n",
    "                else: # return 0 if the colon shows up later\n",
    "                    return 0\n",
    "        # If we get here, there isn't a colon later in the word        \n",
    "        return 0\n",
    "    except:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that we've created these functions, we can continue with the data-cleansing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions are now defined. Will use those later in the apply function \n",
    "\n",
    "df = pd.read_csv('Transcripts_df.csv') # read the transcript data\n",
    "\n",
    "#print(df.columns) # print column names\n",
    "\n",
    "# apply the speaker_map function to isolate the speaker\n",
    "df['Speaker_Clean'] = df.apply(speaker_map, axis = 1)\n",
    "\n",
    "# Need to remove formality from the names\n",
    "l = ['MR. ', 'MRS.', 'MS.']\n",
    "\n",
    "# Replace the formal greetings\n",
    "for i in l:\n",
    "    df['Speaker_Clean'] = df['Speaker_Clean'].str.replace(i, '')\n",
    "\n",
    "# We want to forward fill the speaker column if blank\n",
    "df['Speaker_Clean'] = df['Speaker_Clean'].fillna(method='ffill')\n",
    "\n",
    "# Some steps to cleanse the speaker name even further. If the speaker contains more than three words, we logically\n",
    "# conclude that it isn't actually a speaker, so we set the Speaker_fin column to be the Speaker brought in from the \n",
    "# webscraper\n",
    "\n",
    "df['Speaker_fin'] = np.where(df['Speaker_Clean'].str.split(' ').str.len() > 3, df['Speaker'], df['Speaker_Clean'])\n",
    "df['Speaker_fin'] = np.where(df['Speaker_fin'].isnull(), df['Speaker_Clean'], df['Speaker_fin']) # fill nulls w clean spkr\n",
    "\n",
    "df['word_count'] = df['Transcript'].str.split(' ').str.len() # get total word count, including the speaker\n",
    "df['flag'] = df.apply(find_colon, axis = 1) # apply the find_colon function\n",
    "\n",
    "df['word_count'] = df['word_count'] - df['flag'] # true wordcount is the length of the transcript line minus the flag\n",
    "df = df.drop(['flag', 'Speaker'], axis=1) # cleanup DF by dropping some columns\n",
    "df.fillna('xx') # fill the remaining NAs in the DF with xx\n",
    "\n",
    "#print(df.head())\n",
    "#print(df.dtypes)\n",
    "\n",
    "pd.Series(df['Speaker_fin'].unique()).to_csv('speakers.csv') # used this file to test for abnormal speakers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We've now performed data cleansing on the speakers from the transcript files. We sent the unique elements of the column to a csv. We analyzed those data for any abnormalities as part of our testing and used it as the basis of a mapping file.\n",
    "## Additionally, we created a mapping file containing more data, like party, number of electoral college votes, etc.\n",
    "## This file also cleaned up any abnormalities in the data, such as when a candidate ran for VP and P in different years, and when two candidates have the same last name, like the Bushes or Clintons. We need to specify the year in these instances since the Speaker_fin alone isn't enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_map = pd.read_csv('speaker_map.csv') # read the initial mapping file to clean up the last names\n",
    "clean_map = pd.read_csv('map_file_final.csv') # read in additional data\n",
    "\n",
    "#print(speaker_map.head())\n",
    "#print(speaker_map.dtypes)\n",
    "\n",
    "# this mapping file cleans up any transcripts with abnormalities, like typos or when a transcript was in the speaker\n",
    "# field rather than a name\n",
    "df = df.join(speaker_map.set_index('Speaker_fin'), rsuffix='_map', on = 'Speaker_fin') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We also need to bring in the 2020 debates and append to the rest of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fuent\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "# read in 2020 debates, which don't need the same level of cleansing as the CPD debate transcripts\n",
    "df2 = pd.read_csv('2020debates.csv')\n",
    "df2['word_count'] = df2['Transcript'].str.split(' ').str.len() # compute the word count by spaces\n",
    "\n",
    "df = df.append(df2) # append. All of the transcripts are now in df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that we have a full set of data, the only thing left is to join our additional fields and final speaker identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.columns)\n",
    "\n",
    "# the 2020 debates need slight manipulation to turn the Debate column into a date, so we just nest some replace methods\n",
    "df['Debate_date'] = df['Debate'].str.replace('Debate', '').str.strip().str.replace('Transcripts', '').str.replace('Transcript', '').str.strip().str.replace('First Half', '').str.strip().str.replace('Second Half', '').str.strip()\n",
    "\n",
    "df['Key'] = df['Debate_date'] + ' | ' + df['Speaker_standardized'].str.strip() # create the key for our mapping join\n",
    "\n",
    "df = df.join(clean_map.set_index('Key'), rsuffix='_map', on = 'Key') # join to our mapping table\n",
    "\n",
    "df.drop(['Speaker_fin', '0'], axis = 1, inplace = True) # drop some fields we don't need\n",
    "\n",
    "df['Transcript'] = df['Transcript'].str.upper() # capitalize the transcript text for easier analysis\n",
    "\n",
    "df.to_csv('Transcripts_full.csv') # send the full file to .csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We now have a clean set of transcripts. To help with various analyses, we created different cuts of the data\n",
    "\n",
    "## The following code doesn't need to run -- only run if you want .csv files by year and year & speaker\n",
    "\n",
    "## I also have this in the sentiment analysis to create these files for CANDIDATES ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create yearly files and individual candidate files\n",
    "\n",
    "year_list = df['Year'].unique() # create unique list of years to loop through during file creation\n",
    "speaker_list = df['Actual Speaker'].unique() # do the same for the speaker list\n",
    "\n",
    "abs_dir = os.getcwd() # get current working directory\n",
    "\n",
    "rel_dir = os.path.join(abs_dir, '\\Yearly_Files\\\\') # set up path (I am on a Windows PC)\n",
    "\n",
    "for i in year_list: # create a for loop\n",
    "    if math.isnan(i) == False: # Make sure the iterator didn't pick up an empty year\n",
    "        df_year = df[df['Year'] == i] # filter df for year\n",
    "        df_year.to_csv(abs_dir + '\\Yearly_Files\\Transcripts_' + str(int(i)) + '.csv') # create year file\n",
    "        for j in speaker_list: # do the same for the speakers...\n",
    "            df_y_spk = df_year[df_year['Actual Speaker'] == j] # filter the year file for the speaker\n",
    "            if len(df_y_spk.index) > 0: # make sure there are data in the df (just in case)\n",
    "                \n",
    "                # create year and speaker file\n",
    "                df_y_spk.to_csv(abs_dir + '\\Yearly_Files\\Transcripts_' + j + '_' + str(int(i)) + '.csv') \n",
    "            else:\n",
    "                pass\n",
    "    else:\n",
    "        pass\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
